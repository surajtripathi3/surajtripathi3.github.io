<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Suraj Tripathi | Publications</title>
  <meta name="description" content="Personal website of Suraj Tripathi.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Suraj</span> Tripathi</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    Resume
                    
                  </a>
              </li>
            
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>

              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    Teaching
                    
                  </a>
              </li>

        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publications</h1>
  <h6><nobr><em>*</em></nobr> denotes equal contribution</h6>


<p><br /></p>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <!-- <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div> -->
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  



        <div class="col-sm-1 p-0 abbr">
    
      
          <a class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="width: 70px;" href="https://2022.emnlp.org/" target="_blank">
            EMNLP
          </a>
        
      
    </div>
    <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
      
      <div id="emnlp-22" class="col p-0">
        <h5 class="title mb-0"> Prompt Composition Technique for Code-Switched Tasks</h5>
        <div class="author">
          
          <nobr><em>Suraj Tripathi<nobr><em>*</em></nobr></em>,</nobr>
      
          <nobr><a href="https://scholar.google.ca/citations?user=q-r7dUAAAAAJ&hl=en" target="_blank">Srijan Bansal*</a>,</nobr>

          <nobr><a href="https://www.linkedin.com/in/iamagarwalsumit/" target="_blank">Sumit Agarwal*</a>,</nobr>
            
          <nobr><a href="https://www.cs.cmu.edu/~teruko/" target="_blank">Teruko Mitamura</a>,</nobr>
          
      and
      
          <nobr><a href="https://www.cs.cmu.edu/~ehn/" target="_blank">Eric Nyberg</a>.</nobr>
            
        </div>
  
        <div>
          <p class="periodical font-italic">
            
              
              EMNLP 2022 (long paper)
                        
          </p>
        </div>
  
        <div class="col mt-2 p-0">
          <div id="crosslingual-abstract" class="collapse">
            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
              People speaking different kinds of languages search for information in a cross-lingual manner. They tend to ask questions in their language and expect the answer to be in the same language, despite the evidence lying in another language. In this paper, we present our approach for this task of cross-lingual open-domain question-answering. Our proposed method employs a passage reranker, the fusion-in-decoder technique for generation, and a wiki data entity-based post-processing system to tackle the inability to generate entities across all languages. Our end-2-end pipeline shows an improvement of 3 and 4.6 points on F1 and EM metrics respectively, when compared with the baseline CORA model on the XOR-TyDi dataset. We also evaluate the effectiveness of our proposed techniques in the zero-shot setting using the MKQA dataset and show an improvement of 5 points in F1 for high-resource and 3 points improvement for low-resource zero-shot languages. Our team, CMUmQA’s submission in the MIA-Shared task ranked 1st in the constrained setup for the dev and 2nd in the test setting.      </div>
        </div>
  </div>
        
  
      
    </div> 
  </div>
  </div>

  <div class="col-sm-11 p-0">
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">




      <div class="col-sm-1 p-0 abbr">
  
    
        <a class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="width: 70px;" href="https://mia-workshop.github.io/" target="_blank">
          MIA
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="crosslingual-mia-22" class="col p-0">
      <h5 class="title mb-0">Zero-shot cross-lingual open domain question answering</h5>
      <div class="author">
        
          
            
              
                  <nobr><a href="https://www.linkedin.com/in/iamagarwalsumit/" target="_blank">Sumit Agarwal</a>,</nobr>
                  <nobr><em>Suraj Tripathi<nobr></nobr></em>,</nobr>
                  <nobr><a href="https://www.cs.cmu.edu/~teruko/" target="_blank">Teruko Mitamura</a>,</nobr>
              and
              
                
                  <nobr><a href="https://www.cs.cmu.edu/~cprose/" target="_blank">Carolyn Rose</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Proceedings of the Workshop on Multilingual Information Access (MIA) @ NAACL 
          
          
            2022.
          
        </p>
      </div>

      <div class="col p-0">
        
        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#crosslingual-abstract" role="button" aria-expanded="false" aria-controls="joshi2021dialograph-abstract">Abstract</a>
      
        <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/MIA_CMUmQA.pdf" target="_blank">PDF</a>
    
    </div>

    <div class="col mt-2 p-0">
      <div id="crosslingual-abstract" class="collapse">
        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
          People speaking different kinds of languages search for information in a cross-lingual manner. They tend to ask questions in their language and expect the answer to be in the same language, despite the evidence lying in another language. In this paper, we present our approach for this task of cross-lingual open-domain question-answering. Our proposed method employs a passage reranker, the fusion-in-decoder technique for generation, and a wiki data entity-based post-processing system to tackle the inability to generate entities across all languages. Our end-2-end pipeline shows an improvement of 3 and 4.6 points on F1 and EM metrics respectively, when compared with the baseline CORA model on the XOR-TyDi dataset. We also evaluate the effectiveness of our proposed techniques in the zero-shot setting using the MKQA dataset and show an improvement of 5 points in F1 for high-resource and 3 points improvement for low-resource zero-shot languages. Our team, CMUmQA’s submission in the MIA-Shared task ranked 1st in the constrained setup for the dev and 2nd in the test setting.      </div>
    </div>
  </div>

    
  </div>
</div>
</div>


  <!-- <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;"> -->
    <!-- <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div> -->
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">

        <div class="col-sm-1 p-0 abbr">
    
      
          <a class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="width: 70px;" href="https://mia-workshop.github.io/" target="_blank">
            DialDoc
          </a>
        
      
    </div>
    <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
      
      <div id="dialdoc-22" class="col p-0">
        <h5 class="title mb-0">R3: Refined Retriever-Reader Pipeline for Multidoc2dial</h5>
        <div class="author">
          
            
              
                
          <nobr><em>Suraj Tripathi<nobr><em>*</em></nobr></em>,</nobr>
      
          <nobr><a href="https://scholar.google.ca/citations?user=q-r7dUAAAAAJ&hl=en" target="_blank">Srijan Bansal*</a>,</nobr>

          <nobr><a href="https://www.linkedin.com/in/iamagarwalsumit/" target="_blank">Sumit Agarwal*</a>,</nobr>
            
          <nobr><a href="https://www.linkedin.com/in/sireesh-gururaja-aa629389/" target="_blank">Sireesh Gururaja*</a>,</nobr>

          <nobr><a href="https://aditya-srikanth.github.io/" target="_blank">Aditya Srikanth Veerubhotla*</a>,</nobr>
        
          <nobr><a href="https://scholar.google.co.in/citations?user=iCZtiOsAAAAJ&hl=en" target="_blank">Ritam Dutt</a>,</nobr>

          <nobr><a href="https://www.cs.cmu.edu/~teruko/" target="_blank">Teruko Mitamura</a>,</nobr>
          
      and
      
          <nobr><a href="https://www.cs.cmu.edu/~ehn/" target="_blank">Eric Nyberg</a>.</nobr>

        </div>
  
        <div>
          <p class="periodical font-italic">
            
            In DialDoc @ ACL 
            
            2022.
            
          </p>
        </div>
  
        <div class="col p-0">
          
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#dialdoc-abstract" role="button" aria-expanded="false" aria-controls="joshi2021dialograph-abstract">Abstract</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/R3.pdf" target="_blank">PDF</a>
      
      </div>
  
      <div class="col mt-2 p-0">
        <div id="dialdoc-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            In this paper, we present our submission to the DialDoc shared task based on the MultiDoc2Dial dataset. MultiDoc2Dial is a conversational question answering dataset that grounds dialogues in multiple documents. The task involves grounding a user’s query in a document followed by generating an appropriate response. We propose several improvements over the baseline’s retriever-reader architecture to aid in modeling goal-oriented dialogues grounded in multiple documents. Our proposed approach employs sparse representations for passage retrieval, a passage re-ranker, the fusion-in-decoder architecture for generation, and a curriculum learning training paradigm. Our approach shows a 12 points improvement in BLEU score compared to the baseline RAG model.          </div>
           </div>
      </div>
    </div>
  
      
    </div>
  </div>
  </div>




  <div class="col-sm-11 p-0">
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">

      <div class="col-sm-1 p-0 abbr">
  
    
        <a class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="width: 70px;" href="https://dl.acm.org/doi/proceedings/10.1145/3428690" target="_blank">
          MoMM
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="dialdoc-22" class="col p-0">
      <h5 class="title mb-0">Input-conditioned Convolution Filters for Feature Learning</h5>
      <div class="author">
        
          
            


        <nobr><em>Suraj Tripathi<nobr></nobr></em>,</nobr>

        <nobr><a href="https://www.linkedin.com/in/saurabhtripathi-iitd/?originalSubdomain=in" target="_blank">Saurabh Tripathi</a>,</nobr>

        <nobr><a href="https://abhayk1201.github.io/" target="_blank">Abhay Kumar</a>,</nobr>
    
    and
    
    <nobr><a href="https://scholar.google.com/citations?user=9Lzf7REAAAAJ&hl=en" target="_blank">Chirag Singh</a>,</nobr>

    
      </div>

      <div>
        <p class="periodical font-italic">
          
          In Proceedings of the 18th International Conference on Advances in Mobile Computing & Multimedia
          
          2020.
          
        </p>
      </div>


      <div class="col p-0">
        
        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#momm-abstract" role="button" aria-expanded="false" aria-controls="joshi2021dialograph-abstract">Abstract</a>
      
        <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/momm.pdf" target="_blank">PDF</a>
    
    </div>

    <div class="col mt-2 p-0">
      <div id="momm-abstract" class="collapse">
        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
          We propose a novel framework which combines the input-conditioned filter generation module and a decoder based network to incorpo- rate contextual information present in images into Convolutional Neural Networks (CNNs). In contrast to traditional CNNs, we do not employ the same set of learned convolution filters for all input image instances. And our proposed decoder network serves the purpose of reducing the transformation present in the input image by learning to construct a representative image of the input image class. Our proposed joint supervision of input-aware framework when combined with techniques inspired by Multi-instance learn- ing and max-pooling, results in a transformation-invariant neural network. We investigated the performance of our proposed frame- work on three MNIST variations, which covers both rotation and scaling variance, and achieved 0.98% error on MNIST-rot-12k, 1.12% error on Half-rotated MNIST and 0.68% error on Scaling MNIST, which is significantly better than the state-of-the-art results. Our proposed model also showcased consistent improvement on the CIFAR dataset. We make use of visualization to further prove the effectiveness of our input-aware convolution filters. Our proposed convolution filter generation framework can also serve as a plugin for any CNN based architecture and enhance its modeling capacity. 
        </div>    
      </div>
  </div>

    
  </div>
</div>
</div>



  <div class="col-sm-11 p-0">
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">

      <div class="col-sm-1 p-0 abbr">
  
    
        <a class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="width: 70px;" href="https://interspeech2018.org/" target="_blank">
          WASSA
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="stance-19" class="col p-0">
      <h5 class="title mb-0">	Stance Detection in Code-Mixed Hindi-English Social Media Data using Multi-Task Learning</h5>
      <div class="author">
        
          
        <nobr><em>Suraj Tripathi<nobr><em>*</em></nobr></em>,</nobr>
        <nobr><a href="https://www.linkedin.com/in/sushmithasane/?originalSubdomain=in" target="_blank">Sushmitha Reddy Sane*</a>,</nobr>
        <nobr><a href="https://www.linkedin.com/in/koushik-reddy-sane-484533152/?trk=public_profile_browsemap&originalSubdomain=in" target="_blank">Koushik Reddy Sane</a>,</nobr>

        
    and
    
        <nobr><a href="https://scholar.google.co.in/citations?user=DsIpZR0AAAAJ&hl=en" target="_blank">Radhika Mamidi</a>.</nobr>

      </div>

      <div>
        <p class="periodical font-italic">
          
          In Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, WASSA @ NAACL-HLT
          
          2019.
          
        </p>
      </div>


      <div class="col p-0">
        
        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#wassa-abstract" role="button" aria-expanded="false" aria-controls="joshi2021dialograph-abstract">Abstract</a>
      
        <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/W19-1301/" target="_blank">PDF</a>
    
    </div>

    <div class="col mt-2 p-0">
      <div id="wassa-abstract" class="collapse">
        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
Social media sites like Facebook, Twitter, and other microblogging forums have emerged as a platform for people to express their opinions and views on different issues and events. It is often observed that people tend to take a stance; in favor, against or neutral towards a particular topic. The task of assessing the stance taken by the individual became significantly important with the emergence in the usage of online social platforms. Automatic stance detection system understands the user’s stance by analyzing the standalone texts against a target entity. Due to the limited contextual information a single sentence provides, it is challenging to solve this task effectively. In this paper, we introduce a Multi-Task Learning (MTL) based deep neural network architecture for automatically detecting stance present in the code-mixed corpus. We apply our approach on Hindi-English code-mixed corpus against the target entity - “Demonetisation.” Our best model achieved the result with a stance prediction accuracy of 63.2% which is a 4.5% overall accuracy improvement compared to the current supervised classification systems developed using the benchmark dataset for code-mixed data stance detection.    </div>
  </div>
      </div>

    
  </div>
</div>
</div>
      
      
  <div class="col-sm-11 p-0">
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">

      <div class="col-sm-1 p-0 abbr">
  
    
        <a class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="width: 70px;" href="https://interspeech2018.org/" target="_blank">
          Interspeech
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="dialdoc-22" class="col p-0">
      <h5 class="title mb-0">Speech Emotion Recognition Using Spectrogram & Phoneme Embedding</h5>
      <div class="author">
        
          
            
        <nobr><a href="https://www.linkedin.com/in/pramod-yenigalla/?originalSubdomain=in" target="_blank">Promod Yenigalla</a>,</nobr>
        <nobr><a href="https://abhayk1201.github.io/" target="_blank">Abhay Kumar</a>,</nobr>

        <nobr><em>Suraj Tripathi<nobr></nobr></em>,</nobr>
    
        <nobr><a href="https://scholar.google.com/citations?user=9Lzf7REAAAAJ&hl=en" target="_blank">Chirag Singh</a>,</nobr>

        <nobr><a href="https://www.linkedin.com/in/sibsambhu-kar-90a24a33/?originalSubdomain=in" target="_blank">Sibsambhu Kar</a>,</nobr>
        
    and
    
        <nobr><a href="https://scholar.google.fi/citations?hl=en&user=88HQwhAAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Jithendra Vepa</a>.</nobr>

      </div>

      <div>
        <p class="periodical font-italic">
          
          In INTERSPEECH
          
          2018.
          
        </p>
      </div>


      <div class="col p-0">
        
        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#interspeech-abstract" role="button" aria-expanded="false" aria-controls="joshi2021dialograph-abstract">Abstract</a>
      
        <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/interspeech.pdf" target="_blank">PDF</a>
    
    </div>

    <div class="col mt-2 p-0">
      <div id="interspeech-abstract" class="collapse">
        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
          This paper proposes a speech emotion recognition method based on phoneme sequence and spectrogram. Both phoneme sequence and spectrogram retain emotion contents of speech which is missed if the speech is converted into text. We performed various experiments with different kinds of deep neural networks with phoneme and spectrogram as inputs. Three of those network architectures are presented here that helped to achieve better accuracy when compared to the stateof-the-art methods on benchmark dataset. A phoneme and spectrogram combined CNN model proved to be most accurate in recognizing emotions on IEMOCAP data. We achieved more than 4% increase in overall accuracy and average class accuracy as compared to the existing state-of-the-art methods.         </div>
    </div>
  </div>

    
  </div>
</div>
</div>



  <!-- Footer -->
  <footer>
    &copy; Copyright 2022 Suraj Tripathi.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-105573982-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
